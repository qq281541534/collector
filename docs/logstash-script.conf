## multiline 插件也可以用于其他类似的堆栈式信息，譬如liunx的内核日志。
input {
    kafka {
        # 订阅的主题规则，app-log-服务名称；也可以使用 'topics => ["app-log-collector", "error-log-collector"]' 这种方式
        topics_pattern => "app-log-collector.*"
        # kafka服务器IP端口
        bootstrap_servers => "192.168.31.8:9095"
        codec => json
        consumer_threads => 1 ## 增加consumer的并行消费线程数，该配置对应kafka中topic的partition分区的个数
        decorate_events => true
        #auto_offset_rest => "latest" 默认配置
        group_id => "app-log-group"
    }

    kafka {
        ## 订阅的主题规则，error-log-服务名称；也可以使用 'topics => ["app-log-collector"]' 这种方式
        topics_pattern => "error-log-collector.*"
        ## kafka服务器IP端口
        bootstrap_servers => "192.168.31.8:9095"
        codec => json ##数据json化才能读取到message数据
        consumer_threads => 1 ## 增加consumer的并行消费线程数，该配置对应kafka中topic的partition分区的个数
        decorate_events => true ##decorate_events开启后用mutate才可以拿到topic字段
        #auto_offset_rest => "latest" 默认配置
        group_id => "error-log-group"
    }
}

filter {

    ## 时区转换，因为我们的时区为东八区，所以要+8小时，否则会将第二天前八小时的日志归与前一天的日志文件中
    ruby {
        code => "event.set('index_time', (event.get('@timestamp').time.localtime + 8*60*60).strftime('%Y.%m.%d'))"
    }

    ## 匹配 filebeat.yml中自定义的字段 
    if "app-log" in [fields][logtopic] {
        grok {
            ## 表达式 匹配应用程序中的日志输出格式
            match => ["message", "\[%{NOTSPACE:currentDateTime}\] \[%{NOTSPACE:level}\] \[%{NOTSPACE:thread-id}\] \[%{NOTSPACE:class}\] \[%{DATA:hostName}\] \[%{DATA:ip}\] \[%{DATA:applicationName}\] \[%{DATA:location}\] \[%{DATA:messageInfo}\] ## (\'\'|%{QUOTEDSTRING:throwable})"]
        }
    }

    if "error-log" in [fields][logtopic] {
        grok {
            ## 表达式
            match => ["message", "\[%{NOTSPACE:currentDateTime}\] \[%{NOTSPACE:level}\] \[%{NOTSPACE:thread-id}\] \[%{NOTSPACE:class}\] \[%{DATA:hostName}\] \[%{DATA:ip}\] \[%{DATA:applicationName}\] \[%{DATA:location}\] \[%{DATA:messageInfo}\] ## (\'\'|%{QUOTEDSTRING:throwable})"]
        }
    }
}

## 测试输出到控制台：
output {
    stdout { codec => rubydebug }
}

## elasticsearch
output {

    if "app-log" in [fields][logtopic] {
        elasticsearch {
            ## es服务地址
            hosts => ["192.168.31.8:9200"]
            ## 用户名密码
            user => "elastic"
            password => "123456"
            ## %{[fields][logbiz]} 对应的是filebeat.yml中配置的
            ## %{index_time} 对应的是filter中ruby脚本时区转换的函数
            ## app-log-collector-2020.04.05
            index => "app-log-%{[fields][logbiz]}-%{index_time}"
            ## 是否嗅探集群IP：一般设置true; http://192.168.31.8:9200/_nodes/http?pretty
            ## 通过嗅探机制进行es集群负载均衡发日志消息
            sniffing => true
            ## logstash默认自带一个mapping模板，进行模板覆盖
            template_overwrite => true
        }
    }

    if "error-log" in [fields][logtopic] {
        elasticsearch {
            ## es服务地址
            hosts => ["192.168.31.8:9200"]
            ## 用户名密码
            user => "elastic"
            password => "123456"
            ## %{[fields][logbiz]} 对应的是filebeat.yml中配置的
            ## %{index_time} 对应的是filter中ruby脚本时区转换的函数
            ## app-log-collector-2020.04.05
            index => "error-log-%{[fields][logbiz]}-%{index_time}"
            ## 是否嗅探集群IP：一般设置true; http://192.168.31.8:9200/_nodes/http?pretty
            ## 通过嗅探机制进行es集群负载均衡发日志消息
            sniffing => true
            ## logstash默认自带一个mapping模板，进行模板覆盖
            template_overwrite => true
        }
    }
}