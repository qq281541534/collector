filebeat.prospectors:

- input_type: log
  paths:
    ## app-服务名称.log，写死防止发生轮转抓取历史数据
    - /usr/share/filebeat/logs/app-collector.log
  # 定义写入ES时的_type值
  document_type: "app-log"
  multiline:
    # pattern: 指定匹配的表达式
    pattern: '^\['      # 指定匹配的表达式（匹配以 [ 开头的字符串）
    negate: true        # 是否匹配到
    match: after        # 合并到上一行的末尾
    max_lines: 2000     # 最大的行数
    timeout: 2s         # 如果在规定时间没有新的日志事件就不等待后面的日志输入了，进行推送给其他中间件
  # 自定义的字段  
  fields: 
    logbiz: collector
    logtopic: app-log-collector     ## 按服务划分，用作kafka topic
    evn: dev

- input_type: log
  paths:
    - /usr/share/filebeat/logs/error-collector.log
  # 定义写入ES时的_type值
  document_type: "error-log"
  multiline:
    # pattern: 指定匹配的表达式
    pattern: '^\['      # 指定匹配的表达式（匹配以 [ 开头的字符串）
    negate: true        # 是否匹配到
    match: after        # 合并到上一行的末尾
    max_lines: 2000     # 最大的行数
    timeout: 2s         # 如果在规定时间没有新的日志事件就不等待后面的日志输入了，进行推送给其他中间件
  # 自定义的字段  
  fields: 
    logbiz: collector
    logtopic: error-log-collector     ## 按服务划分，用作kafka topic
    evn: dev  

# 输出到kafka, 也可以输出至其他中间件，譬如直接到es
output.kafka:
  enabled: true
  hosts: ["192.168.31.8:9095"]
  topic: '%{[fields.logtopic]}'
  # 分区规则
  partition.hash:
    reachable_only: true
  # 压缩方式
  compression: gzip
  # 消息的最大字节数10M
  max_message_bytes: 1000000
  # 生产者把消息放松到kafka，消费者端在消费之后，必须手动ack通知kafka消费成功，否则该消息一直算未消费
  required_acks: 1
logging.to_files: true


